{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "def extract_wb_data(request):\n",
    "    # Definir los países, indicadores y años\n",
    "    countries = [\"ARG\", \"BOL\", \"BRA\", \"CAN\", \"CHL\", \"COL\", \"CRI\", \"CUB\", \"DOM\", \"ECU\", \"SLV\", \"GTM\", \"HTI\", \"HND\", \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"USA\", \"URY\", \"VEN\"]\n",
    "    indicators = ['EN.ATM.CO2E.PP.GD', 'SP.DYN.LE00.IN', 'SH.H2O.SMDW.ZS', 'SP.RUR.TOTL.ZS', 'SP.DYN.CDRT.IN','SH.XPD.GHED.GD.ZS','SP.DYN.LE00.FE.IN','SE.ADT.LITR.ZS','SP.DYN.AMRT.FE', 'SP.DYN.AMRT.MA', 'SP.DYN.IMRT.IN','NY.GDP.PCAP.PP.KD', 'SP.DYN.CBRT.IN']\n",
    "    start_year = \"1990\"\n",
    "    end_year = \"2021\"\n",
    "\n",
    "    # Lista para almacenar los datos\n",
    "    data_frames = []\n",
    "\n",
    "    # URL base de la API del Banco Mundial\n",
    "    base_url = \"http://api.worldbank.org/v2/country\"\n",
    "\n",
    "    # Realizar las consultas para cada país, indicador y año\n",
    "    for country_code in countries:\n",
    "        for indicator in indicators:\n",
    "            # Construir la URL de la consulta\n",
    "            url = f\"{base_url}/{country_code}/indicator/{indicator}?date={start_year}:{end_year}&format=json\"\n",
    "\n",
    "            # Realizar la solicitud GET a la API del Banco Mundial\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Verificar si la solicitud fue exitosa\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                if data[1]:\n",
    "                # Los datos se encuentran en data[1]\n",
    "                    for entry in data[1]:\n",
    "                        year = entry['date']\n",
    "                        value = entry['value']\n",
    "                        indicator_name = entry['indicator']['value']\n",
    "                        country_name = entry['country']['value']\n",
    "                        data_frames.append(pd.DataFrame({\"Pais\": [country_name], \"Indicador\": [indicator_name], \"Anio\": [year], \"Valor\": [value]}))\n",
    "                else:\n",
    "                    print(f\"No hay datos disponibles para {country_code} y {indicator}.\")\n",
    "            else:\n",
    "                print(f\"Error al obtener datos para {country_code} y {indicator}. Código de estado: {response.status_code}\")\n",
    "\n",
    "    # Concatenar todos los DataFrames individuales en uno\n",
    "    data_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Exportar el DataFrame a un archivo CSV\n",
    "    csv_data = data_df.to_csv(index=False)\n",
    "\n",
    "    # Guardar el archivo CSV en Google Cloud Storage\n",
    "    bucket_name = \"bucket-extraction\"  # Reemplaza con el nombre de tu bucket\n",
    "    file_name = \"wb_data.csv\"  # Nombre del archivo CSV en el bucket\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    try:\n",
    "        blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "        return \"Datos extraídos y almacenados en Google Cloud Storage.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al cargar el archivo en Google Cloud Storage: {str(e)}\"\n",
    "\n",
    "    #blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "\n",
    "    return \"Datos extraídos y almacenados en Google Cloud Storage.\"\n",
    "\n",
    "#requests\n",
    "#pandas\n",
    "#google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask import jsonify, abort, Request\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mark_outliers_with_nan(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = ((df[column] < lower_whisker) | (df[column] > upper_whisker))\n",
    "    df.loc[outliers_mask, column] = np.nan\n",
    "    return df\n",
    "\n",
    "def imputation_for_all_indicators(df):\n",
    "    for column in df.columns[2:]:\n",
    "        df = linear_regression_imputation(df, column)\n",
    "    return df\n",
    "\n",
    "def linear_regression_imputation(df, column):\n",
    "    df_subset = df[df[column].notna()]\n",
    "    if len(df_subset) > 2:\n",
    "        X_train = df_subset[['Anio']].values.reshape(-1, 1)\n",
    "        y_train = df_subset[column].values\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        df_missing = df[df[column].isna()]\n",
    "        if not df_missing.empty:\n",
    "            X_missing = df_missing[['Anio']].values.reshape(-1, 1)\n",
    "            predictions = model.predict(X_missing)\n",
    "            df.loc[df_missing.index, column] = predictions\n",
    "    return df\n",
    "\n",
    "def process_csv(request: Request):\n",
    "    try:\n",
    "        # Configura el cliente de Google Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Especifica el nombre del bucket y el archivo CSV\n",
    "        bucket_name = 'bucket-extraction'\n",
    "        file_name = 'wb_data.csv'\n",
    "\n",
    "        # Obtiene el bucket y el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "\n",
    "        # Descarga el archivo CSV a un objeto BytesIO\n",
    "        csv_data = blob.download_as_text()\n",
    "\n",
    "        # Crea un DataFrame a partir del CSV\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Transformaciones adicionales\n",
    "        pivoted_df = df.pivot(index=['Pais', 'Anio'], columns='Indicador', values='Valor').reset_index()\n",
    "        pivoted_df.drop_duplicates(inplace=True)\n",
    "        pivoted_df.replace(0, pd.NA, inplace=True)\n",
    "\n",
    "        # Detección y manejo de outliers\n",
    "        for column in pivoted_df.select_dtypes(include=np.number).columns:\n",
    "            pivoted_df = mark_outliers_with_nan(pivoted_df, column)\n",
    "\n",
    "        # Imputación de valores nulos\n",
    "        pivoted_df = imputation_for_all_indicators(pivoted_df)\n",
    "\n",
    "        # Guardar el DataFrame resultante como un nuevo archivo CSV en el bucket\n",
    "        new_file_name = 'processed_data.csv'\n",
    "        new_blob = bucket.blob(new_file_name)\n",
    "        csv_content = pivoted_df.to_csv(index=False)\n",
    "        new_blob.upload_from_string(csv_content)\n",
    "\n",
    "        return jsonify({\"success\": True})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        abort(500, description=str(e))\n",
    "\n",
    "#google-cloud-storage==2.13.0\n",
    "#pandas==2.1.3\n",
    "#numpy==1.26.1\n",
    "#scikit-learn==0.24.2  # Specify the version of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformación tercer sprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask import jsonify, abort, Request\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mark_outliers_with_nan(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = ((df[column] < lower_whisker) | (df[column] > upper_whisker))\n",
    "    df.loc[outliers_mask, column] = np.nan\n",
    "    return df\n",
    "\n",
    "def imputation_for_all_indicators(df):\n",
    "    for column in df.columns[2:]:\n",
    "        df = linear_regression_imputation(df, column)\n",
    "    return df\n",
    "\n",
    "def linear_regression_imputation(df, column):\n",
    "    df_subset = df[df[column].notna()]\n",
    "    if len(df_subset) > 2:\n",
    "        X_train = df_subset[['Anio']].values.reshape(-1, 1)\n",
    "        y_train = df_subset[column].values\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        df_missing = df[df[column].isna()]\n",
    "        if not df_missing.empty:\n",
    "            X_missing = df_missing[['Anio']].values.reshape(-1, 1)\n",
    "            predictions = model.predict(X_missing)\n",
    "            df.loc[df_missing.index, column] = predictions\n",
    "    return df\n",
    "\n",
    "def process_csv(request: Request):\n",
    "    try:\n",
    "        # Configura el cliente de Google Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Especifica el nombre del bucket y el archivo CSV\n",
    "        bucket_name = 'bucket-extraction'\n",
    "        file_name = 'wb_data_machine_learning.csv'\n",
    "\n",
    "        # Obtiene el bucket y el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "\n",
    "        # Descarga el archivo CSV a un objeto BytesIO\n",
    "        csv_data = blob.download_as_text()\n",
    "\n",
    "        # Crea un DataFrame a partir del CSV\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Transformaciones adicionales\n",
    "        pivoted_df = df.pivot(index=['Pais', 'Anio'], columns='Indicador', values='Valor').reset_index()\n",
    "        pivoted_df.drop_duplicates(inplace=True)\n",
    "        pivoted_df.replace(0, pd.NA, inplace=True)\n",
    "\n",
    "        # Detección y manejo de outliers\n",
    "        for column in pivoted_df.select_dtypes(include=np.number).columns:\n",
    "            pivoted_df = mark_outliers_with_nan(pivoted_df, column)\n",
    "\n",
    "        # Imputación de valores nulos\n",
    "        pivoted_df = imputation_for_all_indicators(pivoted_df)\n",
    "\n",
    "        # Renombrar columnas\n",
    "        pivoted_df.rename(columns={'Pais': 'Pais',\n",
    "                    'Anio': 'Anio',\n",
    "                    'Birth rate, crude (per 1,000 people)': 'Tasa_Natalidad',\n",
    "                    'CO2 emissions (kg per PPP $ of GDP)': 'Emisiones_CO2',\n",
    "                    'Compulsory education, duration (years)':'Educacion_obligatoria_en_anios',\n",
    "                    'Death rate, crude (per 1,000 people)': 'Tasa_Mortalidad',\n",
    "                    'Domestic general government health expenditure (% of GDP)': 'Gasto_Salud_Gobierno',\n",
    "                    'GDP per capita, PPP (constant 2017 international $)': 'PIB_per_capita',\n",
    "                    'Life expectancy at birth, female (years)': 'Esperanza_vida_femenina',\n",
    "                    'Life expectancy at birth, total (years)': 'Esperanza_vida_total',\n",
    "                    'Literacy rate, adult total (% of people ages 15 and above)': 'Tasa_alfabetizacion_adultos',\n",
    "                    'Mortality rate, adult, female (per 1,000 female adults)': 'Mortalidad_adulta_femenina',\n",
    "                    'Mortality rate, adult, male (per 1,000 male adults)': 'Mortalidad_adulta_masculina',\n",
    "                    'Mortality rate, infant (per 1,000 live births)': 'Mortalidad_infantil',\n",
    "                    'People using safely managed drinking water services (% of population)': 'Acceso_agua_potable',\n",
    "                    'Rural population (% of total population)': 'Poblacion_rural',\n",
    "\n",
    "                    'Current health expenditure per capita, PPP (current international $)': 'gasto_salud_per_capita_ppp',\n",
    "                    'Domestic general government health expenditure per capita (current US$)': 'gasto_salud_gobierno_per_capita_ppp',\n",
    "                    'Domestic private health expenditure (% of current health expenditure)': 'gasto_salud_privado_pct_gasto_salud_actual',\n",
    "                    'Educational attainment, at least completed lower secondary, population 25+, total (%) (cumulative)': 'logro_educativo_secundaria_inferior_pct_poblacion_25_anios_mas',\n",
    "                    'Gini index': 'indice_gini',\n",
    "                    'Government expenditure on education, total (% of GDP)': 'gasto_educacion_gobierno_pct_pib',\n",
    "                    'Mortality caused by road traffic injury (per 100,000 population)': 'tasa_mortalidad_lesiones_trafico',\n",
    "                    'Mortality from CVD, cancer, diabetes or CRD between exact ages 30 and 70 (%)': 'mortalidad_enfermedades',\n",
    "                    'People using at least basic sanitation services (% of population)': 'personas_saneamiento_basico_pct_poblacion',\n",
    "                    'PM2.5 air pollution, population exposed to levels exceeding WHO guideline value (% of total)': 'contaminacion_pct_poblacion_excede_oms',\n",
    "                    'Political Stability and Absence of Violence/Terrorism: Estimate': 'estabilidad_politica',\n",
    "                    'Poverty gap at $2.15 a day (2017 PPP) (%)': 'brecha_pobreza_2_15_dolars_a_day',\n",
    "                    'Prevalence of undernourishment (% of population)': 'prevalencia_desnutricion_pct_poblacion',\n",
    "                    'Unemployment, total (% of total labor force) (modeled ILO estimate)': 'desempleo_total_ilo',\n",
    "                    'Urban population': 'poblacion_urbana'}, inplace=True)\n",
    "\n",
    "        pivoted_df['Pais'] = pivoted_df['Pais'].replace('Venezuela, RB', 'Venezuela')\n",
    "        \n",
    "        # Guardar el DataFrame resultante como un nuevo archivo CSV en el bucket\n",
    "        new_file_name = 'processed_data_machine_learning.csv'\n",
    "        new_blob = bucket.blob(new_file_name)\n",
    "        csv_content = pivoted_df.to_csv(index=False)\n",
    "        new_blob.upload_from_string(csv_content)\n",
    "\n",
    "        return jsonify({\"success\": True})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        abort(500, description=str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "def trigger_wb_data(request):\n",
    "    # Definir los países, indicadores y años\n",
    "    countries = [\"ARG\", \"BOL\", \"BRA\", \"CAN\", \"CHL\", \"COL\", \"CRI\", \"CUB\", \"DOM\", \"ECU\", \"SLV\", \"GTM\", \"HTI\", \"HND\", \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"USA\", \"URY\", \"VEN\"]\n",
    "    indicators = ['EN.ATM.CO2E.PP.GD', 'SP.DYN.LE00.IN', 'SH.H2O.SMDW.ZS', 'SP.RUR.TOTL.ZS', 'SP.DYN.CDRT.IN','SH.XPD.GHED.GD.ZS','SP.DYN.LE00.FE.IN','SE.ADT.LITR.ZS','SP.DYN.AMRT.FE', 'SP.DYN.AMRT.MA', 'SP.DYN.IMRT.IN','NY.GDP.PCAP.PP.KD', 'SP.DYN.CBRT.IN']\n",
    "    start_year = \"2022\"\n",
    "\n",
    "    # Lista para almacenar los datos\n",
    "    data_frames = []\n",
    "\n",
    "    # URL base de la API del Banco Mundial\n",
    "    base_url = \"http://api.worldbank.org/v2/country\"\n",
    "\n",
    "    # Realizar las consultas para cada país, indicador y año\n",
    "    for country_code in countries:\n",
    "        for indicator in indicators:\n",
    "            # Construir la URL de la consulta\n",
    "            url = f\"{base_url}/{country_code}/indicator/{indicator}?date={start_year}&format=json\"\n",
    "\n",
    "            # Realizar la solicitud GET a la API del Banco Mundial\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Verificar si la solicitud fue exitosa\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                if data[1]:\n",
    "                # Los datos se encuentran en data[1]\n",
    "                    for entry in data[1]:\n",
    "                        year = entry['date']\n",
    "                        value = entry['value']\n",
    "                        indicator_name = entry['indicator']['value']\n",
    "                        country_name = entry['country']['value']\n",
    "                        data_frames.append(pd.DataFrame({\"Pais\": [country_name], \"Indicador\": [indicator_name], \"Anio\": [year], \"Valor\": [value]}))\n",
    "                else:\n",
    "                    print(f\"No hay datos disponibles para {country_code} y {indicator}.\")\n",
    "            else:\n",
    "                print(f\"Error al obtener datos para {country_code} y {indicator}. Código de estado: {response.status_code}\")\n",
    "\n",
    "    # Concatenar todos los DataFrames individuales en uno\n",
    "    data_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Convertir DataFrame a formato CSV en memoria sin incluir encabezados ni índices\n",
    "    csv_content = data_df.to_csv(index=False, header=False)\n",
    "\n",
    "    # Configurar el cliente de Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Especificar el nombre del archivo CSV en tu bucket\n",
    "    bucket_name = 'bucket-extraction'\n",
    "    file_name = 'wb_data.csv'\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Verificar si el archivo ya existe en el bucket\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    if blob.exists():\n",
    "        # Descargar el contenido existente del archivo CSV en el bucket\n",
    "        existing_content = blob.download_as_text()\n",
    "\n",
    "        # Verificar si la información del año 2022 ya está en el archivo existente\n",
    "        if '2022' not in existing_content.split('\\n'):\n",
    "            # Agregar una línea vacía si el archivo existente no termina con una línea vacía\n",
    "            if existing_content and not existing_content.endswith('\\n'):\n",
    "                existing_content += '\\n'\n",
    "            \n",
    "            # Concatenar el nuevo contenido con el contenido existente\n",
    "            csv_content = existing_content + csv_content\n",
    "\n",
    "            # Cargar el contenido actualizado en el archivo CSV en el bucket\n",
    "            blob.upload_from_string(csv_content, content_type='text/csv')\n",
    "\n",
    "            print(f\"Información del año 2022 añadida a {bucket_name}/{file_name}\")\n",
    "        else:\n",
    "            print(f\"Información del año 2022 ya existe en {bucket_name}/{file_name}. No se ha añadido nada.\")\n",
    "    else:\n",
    "        # Si el archivo no existe, cargar el contenido del DataFrame como nuevo archivo\n",
    "        blob.upload_from_string(csv_content, content_type='text/csv')\n",
    "\n",
    "        print(f\"Nuevo archivo CSV creado en {bucket_name}/{file_name}\")\n",
    "\n",
    "    return \"Proceso completado exitosamente\"\n",
    "\n",
    "#requests\n",
    "#pandas\n",
    "#google-cloud-storage\n",
    "#fsspec\n",
    "#gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask import jsonify, abort, Request\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mark_outliers_with_nan(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = ((df[column] < lower_whisker) | (df[column] > upper_whisker))\n",
    "    df.loc[outliers_mask, column] = np.nan\n",
    "    return df\n",
    "\n",
    "def imputation_for_all_indicators(df):\n",
    "    for column in df.columns[2:]:\n",
    "        df = linear_regression_imputation(df, column)\n",
    "    return df\n",
    "\n",
    "def linear_regression_imputation(df, column):\n",
    "    df_subset = df[df[column].notna()]\n",
    "    if len(df_subset) > 2:\n",
    "        X_train = df_subset[['Anio']].values.reshape(-1, 1)\n",
    "        y_train = df_subset[column].values\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        df_missing = df[df[column].isna()]\n",
    "        if not df_missing.empty:\n",
    "            X_missing = df_missing[['Anio']].values.reshape(-1, 1)\n",
    "            predictions = model.predict(X_missing)\n",
    "            df.loc[df_missing.index, column] = predictions\n",
    "    return df\n",
    "\n",
    "def process_csv(request: Request):\n",
    "    try:\n",
    "        # Configura el cliente de Google Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Especifica el nombre del bucket y el archivo CSV\n",
    "        bucket_name = 'bucket-extraction'\n",
    "        file_name = 'wb_data.csv'\n",
    "\n",
    "        # Obtiene el bucket y el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "\n",
    "        # Descarga el archivo CSV a un objeto BytesIO\n",
    "        csv_data = blob.download_as_text()\n",
    "\n",
    "        # Crea un DataFrame a partir del CSV\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Transformaciones adicionales\n",
    "        pivoted_df = df.pivot(index=['Pais', 'Anio'], columns='Indicador', values='Valor').reset_index()\n",
    "        pivoted_df.drop_duplicates(inplace=True)\n",
    "        pivoted_df.replace(0, pd.NA, inplace=True)\n",
    "\n",
    "        # Detección y manejo de outliers\n",
    "        for column in pivoted_df.select_dtypes(include=np.number).columns:\n",
    "            pivoted_df = mark_outliers_with_nan(pivoted_df, column)\n",
    "\n",
    "        # Imputación de valores nulos\n",
    "        pivoted_df = imputation_for_all_indicators(pivoted_df)\n",
    "\n",
    "        # Renombrar columnas\n",
    "        pivoted_df.rename(columns={'Pais': 'Pais',\n",
    "                    'Anio': 'Anio',\n",
    "                    'Birth rate, crude (per 1,000 people)': 'Tasa_Natalidad',\n",
    "                    'CO2 emissions (kg per PPP $ of GDP)': 'Emisiones_CO2',\n",
    "                    'Compulsory education, duration (years)':'Educacion_obligatoria_en_años',\n",
    "                    'Death rate, crude (per 1,000 people)': 'Tasa_Mortalidad',\n",
    "                    'Domestic general government health expenditure (% of GDP)': 'Gasto_Salud_Gobierno',\n",
    "                    'GDP per capita, PPP (constant 2017 international $)': 'PIB_per_capita',\n",
    "                    'Life expectancy at birth, female (years)': 'Esperanza_vida_femenina',\n",
    "                    'Life expectancy at birth, total (years)': 'Esperanza_vida_total',\n",
    "                    'Literacy rate, adult total (% of people ages 15 and above)': 'Tasa_alfabetización_adultos',\n",
    "                    'Mortality rate, adult, female (per 1,000 female adults)': 'Mortalidad_adulta_femenina',\n",
    "                    'Mortality rate, adult, male (per 1,000 male adults)': 'Mortalidad_adulta_masculina',\n",
    "                    'Mortality rate, infant (per 1,000 live births)': 'Mortalidad_infantil',\n",
    "                    'People using safely managed drinking water services (% of population)': 'Acceso_agua_potable',\n",
    "                    'Rural population (% of total population)': 'Población_rural'}, inplace=True)\n",
    "\n",
    "        # Cambiar el tipo de dato de la columna \"nombre_columna\" a entero\n",
    "        pivoted_df['Anio'] = pivoted_df['Anio'].astype(str)\n",
    "\n",
    "        # Guardar el DataFrame resultante como un nuevo archivo CSV en el bucket\n",
    "        new_file_name = 'processed_data.csv'\n",
    "        new_blob = bucket.blob(new_file_name)\n",
    "        csv_content = pivoted_df.to_csv(index=False)\n",
    "        new_blob.upload_from_string(csv_content)\n",
    "\n",
    "        return jsonify({\"success\": True})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        abort(500, description=str(e)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "def process_file(event, context):\n",
    "    # Extraer el nombre del archivo del evento de Cloud Storage\n",
    "    file_name = event['processed_data.csv']\n",
    "    bucket_name = event['bucket-extraction']\n",
    "\n",
    "    # Configurar el cliente de Google Cloud Storage\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Descargar el archivo desde Cloud Storage\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    file_content = blob.download_as_text()\n",
    "\n",
    "    # Configurar el cliente de BigQuery\n",
    "    bq_client = bigquery.Client()\n",
    "\n",
    "    # Configurar el nombre de tu conjunto de datos y tabla en BigQuery\n",
    "    dataset_id = 'bucket_extraction_wb_data'\n",
    "    table_id = 'tabla_transformada_prueba'\n",
    "\n",
    "    # Crear una tabla si no existe\n",
    "    dataset_ref = bq_client.dataset(dataset_id)\n",
    "    table_ref = dataset_ref.table(table_id)\n",
    "    \n",
    "    try:\n",
    "        bq_client.get_table(table_ref)\n",
    "    except NotFound:\n",
    "        # Crear la tabla si no existe\n",
    "        schema = [\n",
    "            bigquery.SchemaField('Pais', 'STRING'),\n",
    "            bigquery.SchemaField('Anio', 'INTEGER'),\n",
    "            bigquery.SchemaField('Tasa_Natalidad', 'FLOAT'),\n",
    "            bigquery.SchemaField('Emisiones_CO2', 'FLOAT'),\n",
    "            bigquery.SchemaField('Educacion_obligatoria_en_a__os', 'FLOAT'),\n",
    "            bigquery.SchemaField('Tasa_Mortalidad', 'FLOAT'),\n",
    "            bigquery.SchemaField('Gasto_Salud_Gobierno', 'FLOAT'),\n",
    "            bigquery.SchemaField('PIB_per_capita', 'FLOAT'),\n",
    "            bigquery.SchemaField('Esperanza_vida_femenina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Esperanza_vida_total', 'FLOAT'),\n",
    "            bigquery.SchemaField('Tasa_alfabetizaci__n_adultos', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_adulta_femenina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_adulta_masculina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_infantil', 'FLOAT'),\n",
    "            bigquery.SchemaField('Acceso_agua_potable', 'FLOAT'),\n",
    "            bigquery.SchemaField('Poblaci__n_rural', 'FLOAT'),\n",
    "            # Agrega más campos según tu esquema\n",
    "        ]\n",
    "        table = bigquery.Table(table_ref, schema=schema)\n",
    "        bq_client.create_table(table)\n",
    "\n",
    "    # Cargar datos en BigQuery\n",
    "    job_config = bigquery.LoadJobConfig(\n",
    "        schema=[\n",
    "            bigquery.SchemaField('Pais', 'STRING'),\n",
    "            bigquery.SchemaField('Anio', 'INTEGER'),\n",
    "            bigquery.SchemaField('Tasa_Natalidad', 'FLOAT'),\n",
    "            bigquery.SchemaField('Emisiones_CO2', 'FLOAT'),\n",
    "            bigquery.SchemaField('Educacion_obligatoria_en_a__os', 'FLOAT'),\n",
    "            bigquery.SchemaField('Tasa_Mortalidad', 'FLOAT'),\n",
    "            bigquery.SchemaField('Gasto_Salud_Gobierno', 'FLOAT'),\n",
    "            bigquery.SchemaField('PIB_per_capita', 'FLOAT'),\n",
    "            bigquery.SchemaField('Esperanza_vida_femenina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Esperanza_vida_total', 'FLOAT'),\n",
    "            bigquery.SchemaField('Tasa_alfabetizaci__n_adultos', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_adulta_femenina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_adulta_masculina', 'FLOAT'),\n",
    "            bigquery.SchemaField('Mortalidad_infantil', 'FLOAT'),\n",
    "            bigquery.SchemaField('Acceso_agua_potable', 'FLOAT'),\n",
    "            bigquery.SchemaField('Poblaci__n_rural', 'FLOAT'),\n",
    "            # Agrega más campos según tu esquema\n",
    "        ],\n",
    "        skip_leading_rows=1,\n",
    "        source_format=bigquery.SourceFormat.CSV,\n",
    "    )\n",
    "\n",
    "    job = bq_client.load_table_from_file(file_content, table_ref, job_config=job_config)\n",
    "    job.result()\n",
    "\n",
    "    print(f\"Datos cargados exitosamente en {dataset_id}.{table_id}\")\n",
    "\n",
    "    return \"Proceso completado con éxito.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "\n",
    "def cargar_datos(request):\n",
    "    # Inicializa BigQuery y Storage\n",
    "    client = bigquery.Client()\n",
    "    storage_client = storage.Client()\n",
    "\n",
    "    # Configuración del archivo y el bucket\n",
    "    bucket_name = 'bucket-extraction'\n",
    "    file_name = 'processed_data.csv'\n",
    "\n",
    "    # Configuración del conjunto de datos y la tabla en BigQuery\n",
    "    dataset_id = 'bucket_extraction_wb_data'\n",
    "    table_id = 'tabla_transformada_prueba'\n",
    "    table_full_id = f\"{client.project}.{dataset_id}.{table_id}\"\n",
    "\n",
    "    # Obtén la información del archivo desde el bucket\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    file_contents = blob.download_as_text()\n",
    "\n",
    "    # Parsea el contenido del archivo como JSON\n",
    "    json_data = [line.split(',') for line in file_contents.split('\\n')]\n",
    "    field_names = json_data[0]\n",
    "    json_data = [dict(zip(field_names, line)) for line in json_data[1:]]\n",
    "\n",
    "    # Verifica si la tabla existe y, si no, la crea\n",
    "    try:\n",
    "        client.get_table(table_full_id)\n",
    "    except Exception as e:\n",
    "        if \"Not found\" in str(e):\n",
    "            schema = [\n",
    "                bigquery.SchemaField(\"Pais\", \"STRING\"),\n",
    "                bigquery.SchemaField(\"Anio\", \"INTEGER\"),\n",
    "                bigquery.SchemaField(\"Tasa_Natalidad\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Emisiones_CO2\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Educacion_obligatoria_en_anos\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Tasa_Mortalidad\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Gasto_Salud_Gobierno\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"PIB_per_capita\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Esperanza_vida_femenina\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Esperanza_vida_total\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Tasa_alfabetizacion_adultos\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Mortalidad_adulta_femenina\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Mortalidad_adulta_masculina\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Mortalidad_infantil\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Acceso_agua_potable\", \"FLOAT64\"),\n",
    "                bigquery.SchemaField(\"Poblacion_rural\", \"FLOAT64\"),\n",
    "            ]\n",
    "            table = bigquery.Table(table_full_id, schema=schema)\n",
    "            client.create_table(table)\n",
    "\n",
    "    # Carga los datos en BigQuery\n",
    "    table_ref = client.dataset(dataset_id).table(table_id)\n",
    "    table = client.get_table(table_ref)\n",
    "\n",
    "    errors = client.insert_rows_json(table, json_data)\n",
    "\n",
    "    if errors:\n",
    "        return f'Error al cargar datos a BigQuery: {errors}'\n",
    "    else:\n",
    "        return 'Datos cargados exitosamente a BigQuery.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
