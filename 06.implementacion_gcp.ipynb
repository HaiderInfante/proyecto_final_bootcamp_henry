{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo Extracción"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "def extract_wb_data(request):\n",
    "    # Definir los países, indicadores y años\n",
    "    countries = [\"ARG\", \"BOL\", \"BRA\", \"CAN\", \"CHL\", \"COL\", \"CRI\", \"CUB\", \"DOM\", \"ECU\", \"SLV\", \"GTM\", \"HTI\", \"HND\", \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"USA\", \"URY\", \"VEN\"]\n",
    "    indicators = ['EN.ATM.CO2E.PP.GD', 'SP.DYN.LE00.IN', 'SH.H2O.SMDW.ZS', 'SP.RUR.TOTL.ZS', 'SP.DYN.CDRT.IN','SH.XPD.GHED.GD.ZS','SP.DYN.LE00.FE.IN','SE.ADT.LITR.ZS','SP.DYN.AMRT.FE', 'SP.DYN.AMRT.MA', 'SP.DYN.IMRT.IN','NY.GDP.PCAP.PP.KD', 'SP.DYN.CBRT.IN']\n",
    "    start_year = \"1990\"\n",
    "    end_year = \"2021\"\n",
    "\n",
    "    # Lista para almacenar los datos\n",
    "    data_frames = []\n",
    "\n",
    "    # URL base de la API del Banco Mundial\n",
    "    base_url = \"http://api.worldbank.org/v2/country\"\n",
    "\n",
    "    # Realizar las consultas para cada país, indicador y año\n",
    "    for country_code in countries:\n",
    "        for indicator in indicators:\n",
    "            # Construir la URL de la consulta\n",
    "            url = f\"{base_url}/{country_code}/indicator/{indicator}?date={start_year}:{end_year}&format=json\"\n",
    "\n",
    "            # Realizar la solicitud GET a la API del Banco Mundial\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Verificar si la solicitud fue exitosa\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                if data[1]:\n",
    "                # Los datos se encuentran en data[1]\n",
    "                    for entry in data[1]:\n",
    "                        year = entry['date']\n",
    "                        value = entry['value']\n",
    "                        indicator_name = entry['indicator']['value']\n",
    "                        country_name = entry['country']['value']\n",
    "                        data_frames.append(pd.DataFrame({\"Pais\": [country_name], \"Indicador\": [indicator_name], \"Anio\": [year], \"Valor\": [value]}))\n",
    "                else:\n",
    "                    print(f\"No hay datos disponibles para {country_code} y {indicator}.\")\n",
    "            else:\n",
    "                print(f\"Error al obtener datos para {country_code} y {indicator}. Código de estado: {response.status_code}\")\n",
    "\n",
    "    # Concatenar todos los DataFrames individuales en uno\n",
    "    data_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Exportar el DataFrame a un archivo CSV\n",
    "    csv_data = data_df.to_csv(index=False)\n",
    "\n",
    "    # Guardar el archivo CSV en Google Cloud Storage\n",
    "    bucket_name = \"bucket-extraction\"  # Reemplaza con el nombre de tu bucket\n",
    "    file_name = \"wb_data.csv\"  # Nombre del archivo CSV en el bucket\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(file_name)\n",
    "    try:\n",
    "        blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "        return \"Datos extraídos y almacenados en Google Cloud Storage.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error al cargar el archivo en Google Cloud Storage: {str(e)}\"\n",
    "\n",
    "    #blob.upload_from_string(csv_data, content_type=\"text/csv\")\n",
    "\n",
    "    return \"Datos extraídos y almacenados en Google Cloud Storage.\"\n",
    "\n",
    "#requests\n",
    "#pandas\n",
    "#google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo transformación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from flask import jsonify, abort, Request\n",
    "from google.cloud import storage\n",
    "import io\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def mark_outliers_with_nan(df, column):\n",
    "    Q1 = df[column].quantile(0.25)\n",
    "    Q3 = df[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_whisker = Q1 - 1.5 * IQR\n",
    "    upper_whisker = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers_mask = ((df[column] < lower_whisker) | (df[column] > upper_whisker))\n",
    "    df.loc[outliers_mask, column] = np.nan\n",
    "    return df\n",
    "\n",
    "def imputation_for_all_indicators(df):\n",
    "    for column in df.columns[2:]:\n",
    "        df = linear_regression_imputation(df, column)\n",
    "    return df\n",
    "\n",
    "def linear_regression_imputation(df, column):\n",
    "    df_subset = df[df[column].notna()]\n",
    "    if len(df_subset) > 2:\n",
    "        X_train = df_subset[['Anio']].values.reshape(-1, 1)\n",
    "        y_train = df_subset[column].values\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        df_missing = df[df[column].isna()]\n",
    "        if not df_missing.empty:\n",
    "            X_missing = df_missing[['Anio']].values.reshape(-1, 1)\n",
    "            predictions = model.predict(X_missing)\n",
    "            df.loc[df_missing.index, column] = predictions\n",
    "    return df\n",
    "\n",
    "def process_csv(request: Request):\n",
    "    try:\n",
    "        # Configura el cliente de Google Cloud Storage\n",
    "        storage_client = storage.Client()\n",
    "\n",
    "        # Especifica el nombre del bucket y el archivo CSV\n",
    "        bucket_name = 'bucket-extraction'\n",
    "        file_name = 'wb_data.csv'\n",
    "\n",
    "        # Obtiene el bucket y el archivo\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(file_name)\n",
    "\n",
    "        # Descarga el archivo CSV a un objeto BytesIO\n",
    "        csv_data = blob.download_as_text()\n",
    "\n",
    "        # Crea un DataFrame a partir del CSV\n",
    "        df = pd.read_csv(io.StringIO(csv_data))\n",
    "\n",
    "        # Transformaciones adicionales\n",
    "        pivoted_df = df.pivot(index=['Pais', 'Anio'], columns='Indicador', values='Valor').reset_index()\n",
    "        pivoted_df.drop_duplicates(inplace=True)\n",
    "        pivoted_df.replace(0, pd.NA, inplace=True)\n",
    "\n",
    "        # Detección y manejo de outliers\n",
    "        for column in pivoted_df.select_dtypes(include=np.number).columns:\n",
    "            pivoted_df = mark_outliers_with_nan(pivoted_df, column)\n",
    "\n",
    "        # Imputación de valores nulos\n",
    "        pivoted_df = imputation_for_all_indicators(pivoted_df)\n",
    "\n",
    "        # Guardar el DataFrame resultante como un nuevo archivo CSV en el bucket\n",
    "        new_file_name = 'processed_data.csv'\n",
    "        new_blob = bucket.blob(new_file_name)\n",
    "        csv_content = pivoted_df.to_csv(index=False)\n",
    "        new_blob.upload_from_string(csv_content)\n",
    "\n",
    "        return jsonify({\"success\": True})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        abort(500, description=str(e))\n",
    "\n",
    "#google-cloud-storage==2.13.0\n",
    "#pandas==2.1.3\n",
    "#numpy==1.26.1\n",
    "#scikit-learn==0.24.2  # Specify the version of scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Codigo trigger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from google.cloud import storage\n",
    "\n",
    "def trigger_wb_data(request):\n",
    "    # Definir los países, indicadores y años\n",
    "    countries = [\"ARG\", \"BOL\", \"BRA\", \"CAN\", \"CHL\", \"COL\", \"CRI\", \"CUB\", \"DOM\", \"ECU\", \"SLV\", \"GTM\", \"HTI\", \"HND\", \"JAM\", \"MEX\", \"NIC\", \"PAN\", \"PRY\", \"PER\", \"USA\", \"URY\", \"VEN\"]\n",
    "    indicators = ['EN.ATM.CO2E.PP.GD', 'SP.DYN.LE00.IN', 'SH.H2O.SMDW.ZS', 'SP.RUR.TOTL.ZS', 'SP.DYN.CDRT.IN','SH.XPD.GHED.GD.ZS','SP.DYN.LE00.FE.IN','SE.ADT.LITR.ZS','SP.DYN.AMRT.FE', 'SP.DYN.AMRT.MA', 'SP.DYN.IMRT.IN','NY.GDP.PCAP.PP.KD', 'SP.DYN.CBRT.IN']\n",
    "    start_year = \"2022\"\n",
    "\n",
    "    # Lista para almacenar los datos\n",
    "    data_frames = []\n",
    "\n",
    "    # URL base de la API del Banco Mundial\n",
    "    base_url = \"http://api.worldbank.org/v2/country\"\n",
    "\n",
    "    # Realizar las consultas para cada país, indicador y año\n",
    "    for country_code in countries:\n",
    "        for indicator in indicators:\n",
    "            # Construir la URL de la consulta\n",
    "            url = f\"{base_url}/{country_code}/indicator/{indicator}?date={start_year}&format=json\"\n",
    "\n",
    "            # Realizar la solicitud GET a la API del Banco Mundial\n",
    "            response = requests.get(url)\n",
    "\n",
    "            # Verificar si la solicitud fue exitosa\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "\n",
    "                if data[1]:\n",
    "                # Los datos se encuentran en data[1]\n",
    "                    for entry in data[1]:\n",
    "                        year = entry['date']\n",
    "                        value = entry['value']\n",
    "                        indicator_name = entry['indicator']['value']\n",
    "                        country_name = entry['country']['value']\n",
    "                        data_frames.append(pd.DataFrame({\"Pais\": [country_name], \"Indicador\": [indicator_name], \"Anio\": [year], \"Valor\": [value]}))\n",
    "                else:\n",
    "                    print(f\"No hay datos disponibles para {country_code} y {indicator}.\")\n",
    "            else:\n",
    "                print(f\"Error al obtener datos para {country_code} y {indicator}. Código de estado: {response.status_code}\")\n",
    "\n",
    "    # Concatenar todos los DataFrames individuales en uno\n",
    "    data_df = pd.concat(data_frames, ignore_index=True)\n",
    "\n",
    "    # Convertir DataFrame a formato CSV en memoria sin incluir encabezados ni índices\n",
    "    csv_content = data_df.to_csv(index=False, header=False)\n",
    "\n",
    "    # Configurar el cliente de Cloud Storage\n",
    "    client = storage.Client()\n",
    "\n",
    "    # Especificar el nombre del archivo CSV en tu bucket\n",
    "    bucket_name = 'bucket-extraction'\n",
    "    file_name = 'wb_data.csv'\n",
    "    \n",
    "    # Obtener el bucket\n",
    "    bucket = client.get_bucket(bucket_name)\n",
    "\n",
    "    # Verificar si el archivo ya existe en el bucket\n",
    "    blob = bucket.blob(file_name)\n",
    "\n",
    "    if blob.exists():\n",
    "        # Descargar el contenido existente del archivo CSV en el bucket\n",
    "        existing_content = blob.download_as_text()\n",
    "\n",
    "        # Verificar si la información del año 2022 ya está en el archivo existente\n",
    "        if '2022' not in existing_content.split('\\n'):\n",
    "            # Agregar una línea vacía si el archivo existente no termina con una línea vacía\n",
    "            if existing_content and not existing_content.endswith('\\n'):\n",
    "                existing_content += '\\n'\n",
    "            \n",
    "            # Concatenar el nuevo contenido con el contenido existente\n",
    "            csv_content = existing_content + csv_content\n",
    "\n",
    "            # Cargar el contenido actualizado en el archivo CSV en el bucket\n",
    "            blob.upload_from_string(csv_content, content_type='text/csv')\n",
    "\n",
    "            print(f\"Información del año 2022 añadida a {bucket_name}/{file_name}\")\n",
    "        else:\n",
    "            print(f\"Información del año 2022 ya existe en {bucket_name}/{file_name}. No se ha añadido nada.\")\n",
    "    else:\n",
    "        # Si el archivo no existe, cargar el contenido del DataFrame como nuevo archivo\n",
    "        blob.upload_from_string(csv_content, content_type='text/csv')\n",
    "\n",
    "        print(f\"Nuevo archivo CSV creado en {bucket_name}/{file_name}\")\n",
    "\n",
    "    return \"Proceso completado exitosamente\"\n",
    "\n",
    "#requests\n",
    "#pandas\n",
    "#google-cloud-storage\n",
    "#fsspec\n",
    "#gcsfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
